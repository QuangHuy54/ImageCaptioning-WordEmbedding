# Image Captioning using different Pretrained Word Embeddings
Conduct the experiment using different pretrained word embeddings with a traditional encoder-decoder and attention mechanism for image captioning

Dataset: [Flickr8k](https://github.com/goodwillyoga/Flickr8k_dataset), [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/) 

Split: [Kaparthy split](https://cs.stanford.edu/people/karpathy/deepimagesent/)

Word embeddings: [GloVe](https://nlp.stanford.edu/projects/glove/), [FastText](https://fasttext.cc)

